Ans 1

The General Linear Model (GLM) is a flexible and powerful statistical framework used for analyzing and modeling relationships between variables.It is used to model the relationship between a dependent variable and one or more independent variables.It is widely used in various fields such as regression analysis, analysis of variance (ANOVA), and analysis of covariance (ANCOVA).In the GLM, the dependent variable is assumed to follow a particular probability distribution like Gaussian, binomial, Poisson,Uniform etc.
Key components of GLM are:
a) Dependent Variable ->  It is a feature whose value needs to be predicted by the machine learning model.It can be a categorical or 
continous variable.
b)Independent Variables -> Also known as predictor variables or covariates, these variables represent the factors that are believed to influence the dependent variable. They can be continuous or categorical.
------------------------------------------------------------------------------------------------------------------------------------

Ans 2

The key assumptions of GLM are:

1. Linearity: It assumes that the relationship between the dependent feature and the independent vfeatures is linear. This means that the effect of each independent variable on the dependent variable is constant across the range of the independent variables.

2. Independence: The records or observation in the dataset should be independent of each other. This assumption implies that there is no systematic relationship or dependency between observations. Violations of this assumption, such as autocorrelation in time series data or clustered observations, can lead to biased and inefficient parameter estimates.

3. Homoscedasticity: Homoscedasticity assumes that the variance of the errors (residuals) is constant across all levels of the independent variables. In other words, the spread of the residuals should be consistent throughout the range of the predictors. Heteroscedasticity, where the variance of the errors varies with the levels of the predictors, violates this assumption and can impact the validity of statistical tests and confidence intervals.

4. Normality: The GLM assumes that the errors or residuals follow a gaussian distribution(bell curve). This assumption is necessary for valid hypothesis testing, confidence intervals, and model inference. Violations of normality can affect the accuracy of parameter estimates and hypothesis tests.

5. No Multicollinearity: Multicollinearity refers to a high degree of correlation between independent variables in the model.There can
be highly positive or highly negative correlation btw independent features. The GLM assumes that the independent variables are not perfectly correlated with each other, as this can lead to instability and difficulty in estimating the individual effects of the predictors.
----------------------------------------------------------------------------------------------------------------------------------------

Ans 3

Interpreting the coefficients in the (GLM) is necessay in order to understand the relationship between the independent variables and the dependent variable. 

1. Coefficient Sign:
The sign (+ or -) of the coefficient indicates the direction of the relationship between the independent variable and the dependent variable. Generally the value ranges btw in [-1 to 1]. A positive coefficient indicates a positive relationship, that means if the value of independent feature is increasing then the value of dependent variable will also increase. Conversely, a negative coefficient indicates a negative relationship, where an increase in the independent variable is associated with a decrease in the dependent variable.

2. Magnitude:
The magnitude of the coefficient reflects the size of the effect that the independent variable has on the dependent variable, all else being equal. Larger coefficient values indicate a stronger influence of the independent variable on the dependent variable. For example, if the coefficient for a variable is 0.8, it means that a one-unit increase in the independent variable is associated with a 0.8-unit increase (or decrease, depending on the sign) in the dependent variable.
----------------------------------------------------------------------------------------------------------------------------------------

Ans 4

The difference btw univariate GLM and multivariate GLM is that there is only one dependent variable in case of univariate GLM and two or
mode dependent vaiables in multivariate GLM. In Univariate GLM the model examines the relationship between this single dependent variable and one or more independent variables (predictor variables). The focus is on understanding how the independent variables influence the variation in the single dependent variable.But in Multivariate GLM the model explores the relationships between these multiple dependent variables and the independent variables.
---------------------------------------------------------------------------------------------------------------------------------------

Ans 5


In a General Linear Model (GLM), interaction effects refer to the situation where the relationship between independent variables (predictors) and the dependent variable (outcome) varies depending on the levels of other independent variables. In other words, an interaction effect suggests that the effect of one independent variable on the dependent variable is not constant across different levels of another independent variable.
----------------------------------------------------------------------------------------------------------------------------------------

Ans 6

Handling categorical variables in GLM is very important.It can be done with the help of various Encoding techniques.These encoding 
techniques are present in sklearn library.

1) Dummy Coding (Binary Encoding):
It is generally used when we have binary classification problem. Dummy coding is widely used technique to handle categorical variables in the GLM. It involves creating binary zero or one dummy variables for each category within the categorical variable. The reference category is represented by 0 values for all dummy variables, while the other categories are encoded with 1 for the corresponding dummy variable.

2) Effect Coding (Deviation Encoding):
Effect coding, also called deviation coding, is another encoding technique for categorical variables in the GLM. In effect coding, each category is represented by a dummy variable, similar to dummy coding. However, unlike dummy coding, the reference category has -1 values for the corresponding dummy variable, while the other categories have 0 or 1 values.

3) One-Hot Encoding:
One-hot encoding is another popular technique for handling categorical variables. It creates a separate binary variable for each category within the categorical variable. Each variable represents whether an observation belongs to a particular category (1) or not (0). One-hot encoding increases the dimensionality of the data, but it ensures that the GLM can capture the effects of each category independently.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 7

The purpose of the design matrix in a GLM:
1. Encoding Independent Variables:
The design matrix represents the independent variables in a structured manner. Each column of the matrix corresponds to a specific independent variable, and each row corresponds to an observation or data point. The design matrix encodes the values of the independent variables for each observation, allowing the GLM to incorporate them into the model.

2. Incorporating Nonlinear Relationships:
The design matrix can include transformations or interactions of the original independent variables to capture nonlinear relationships between the predictors and the dependent variable. For example, polynomial terms, logarithmic transformations, or interaction terms can be included in the design matrix to account for nonlinearities or interactions in the GLM.

3. Handling Categorical Variables:
Categorical variables need to be properly encoded to be included in the GLM. The design matrix can handle categorical variables by using dummy coding or other encoding schemes. Dummy variables are binary variables representing the categories of the original variable. By encoding categorical variables appropriately in the design matrix, the GLM can incorporate them in the model and estimate the corresponding coefficients.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 8

The p-value associated with each predictor indicates the probability of observing the estimated coefficient (or a more extreme value) if the null hypothesis were true. A smaller p-value suggests stronger evidence against the null hypothesis and provides support for the alternative hypothesis. Common significance levels used are 0.05 (5%) or 0.01 (1%), but the choice of significance level depends on the specific research context.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 9

Type I sums of squares: Also known as sequential sums of squares, Type I sums of squares consider the order of entry of predictors into the model. It assesses the unique contribution of each predictor variable after accounting for the effects of previously entered predictors. This means that the sums of squares for a given predictor are based on the change in model fit when that predictor is added last. The order in which the predictors are entered can affect the Type I sums of squares, and different orders can yield different results.

Type II sums of squares: Type II sums of squares do not consider the order of entry of predictors into the model. Instead, they evaluate the unique contribution of each predictor while controlling for the effects of other predictors. Each predictor's sums of squares is calculated while considering all other predictors in the model. Type II sums of squares are useful when there are potentially correlated predictors, as they assess the independent effect of each predictor after accounting for the presence of other predictors in the model.

Type III sums of squares: Type III sums of squares also do not consider the order of entry of predictors into the model. However, unlike Type II sums of squares, they assess the unique contribution of each predictor while controlling for the effects of other predictors, including interaction terms. Type III sums of squares estimate the effect of each predictor in the presence of all other predictors and their interactions. This makes Type III sums of squares useful when there are interaction effects in the model.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 10

In a General Linear Model (GLM), deviance is a measure of the lack of fit between the observed data and the model's predicted values. It is commonly used in GLMs, particularly in logistic regression and other generalized linear models.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 11

Regression analysis is a statistical technique used to find the relationship between a dependent variable and independent variables. Its aim is to find the best fit line. Regression analysis helps in predicting and estimating the values of the dependent variable based on the values of the independent variables. Here are a few examples of regression analysis:

The purpose of Regression algorithm is to find the linear relationship btw independent and dependent feature.Its aim is to find the best
fit line.

Types of Regression

1. Simple Linear Regression:
In Simple linear regression there is only one independent variable (X) and a continuous dependent variable (Y). It finds the relationship between X and Y as a straight line. For example, consider a dataset that contains information about customer salary (X) and their expenditures (Y). Simple linear regression can be used to model how salary of customer impact sales and make predictions about the expenditure of customer.

2. Multiple Linear Regression:
Multiple linear regression involves two or more independent variables (X1, X2, X3, etc.) and a continuous dependent variable (Y). It models the relationship between the independent variables and the dependent variable. Lets consider a dataset that has information about a house's price (Y) based on its attributes such as location (X1), city (X2), and size (X3).

3. Polynomial Regression:
Polynomial regression is an extension of linear regression that models the relationship between the independent variables and the dependent variable as a higher-degree polynomial function. It allows for capturing nonlinear relationships between the variables. For example, consider a dataset that includes information about the age of houses (X) and their corresponding sale prices (Y). Polynomial regression can be used to model how the age of a house affects its sale price and account for potential nonlinearities in the relationship.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 12
The difference btw simple linear regression and multiple regression is that there is only one independent feature in simple linear 
regression and there are more than one independent features in multiple regression.

Simple Linear Regression
The equation in simple linear regression is h$(x)=$0+$1x   where $ represent theta

The objective of simple linear regression is to estimate the values of $0 and $1 that minimize the sum of squared differences between the observed Y values and the predicted Y values based on the regression line. This estimation is typically done using methods like Ordinary Least Squares (OLS).

Multiple Regression
The equation in multiple regression is h$(x)=$0+$1x1+$2x2+$3x3+----+$nxn        where n represent no of indepedent variables.
Y represents the dependent variable.

- X1, X2, X3, ..., Xn represent the independent variables.
- $0, $1, $2, $3, ..., $n represent the coefficients, representing the intercept and the slopes for each independent variable.
- ε represents the error term, accounting for the random variability in Y that is not explained by the linear relationship with the independent variables.

-----------------------------------------------------------------------------------------------------------------------------------------

Ans 13

R-squared is a widely used measure to assess the goodness of fit in regression. It represents the proportion of the variance in the dependent variable that can be explained by the independent variables in the model.The R-squared value is a measure of the proportion of variance in the dependent variable that is explained by the independent variables in a regression model.It quantifies the goodness of fit of the regression model and indicates how well the model captures the variability in the data. The R-squared value ranges from 0 to 1, or as a percentage from 0% to 100%. It represents the proportion of the total variation in the dependent variable that can be accounted for by the independent variables.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 14

Correlation: The objective of correlation is to measure the strength and direction of the linear relationship between two variables. It seeks to determine how closely the variables are associated with each other. A positive coefficient indicates a positive relationship, that means if the value of independent feature is increasing then the value of dependent variable will also increase. Conversely, a negative coefficient indicates a negative relationship, where an increase in the independent variable is associated with a decrease in the dependent variable.enerally the value ranges btw in [-1 to 1].

Regression: Regression aims to model and predict the relationship between a dependent variable and one or more independent variables. It focuses on understanding how changes in the independent variables are related to changes in the dependent variable.It helps in further 
predicting the value of dependent variable.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 15

Intercept: The intercept is the value of the dependent variable when all independent variables are zero.It basically means where does the best fit line meets at y axis when x value is Zero. In a simple linear regression, the intercept represents the value of the dependent variable when the independent variable is zero. In multiple regression, the intercept represents the value of the dependent variable when all the independent variables are zero.

Coefficients: The coefficients, also called regression coefficients or slope coefficients, represent the change in the dependent variable associated with a one-unit change in the corresponding independent variable, while holding other independent variables constant. In simple linear regression, there is only one coefficient that quantifies the change in the dependent variable per unit change in the independent variable. In multiple regression, each independent variable has its own coefficient, indicating the unique effect of that variable on the dependent variable, adjusting for the effects of other variables.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 16

Start by identifying potential outliers in the data.It can be done with the help of boxplot.Determine the reason behind the outliers. They can arise due to genuine extreme values, data collection errors, or other unusual circumstances.If outliers are influencing the regression model but are valid data points, consider transforming the variables to reduce the impact of outliers.In some cases, extreme outliers that are suspected to be erroneous or influential may be removed from the analysis
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 17

Ordinary Least Squares (OLS) regression: OLS assumes that the independent variables are not highly correlated with each other. When multicollinearity is present (high correlation among predictors), OLS regression estimates can become unstable and highly sensitive to small changes in the data. This can lead to unreliable coefficient estimates and incorrect inferences.

Ridge regression: Ridge regression is specifically designed to handle multicollinearity. It adds a penalty term to the OLS objective function, known as the L2 regularization or ridge penalty. This penalty term shrinks the coefficient estimates, reducing the impact of multicollinearity. Ridge regression provides more stable and robust coefficient estimates in the presence of multicollinearity.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 18

Heteroscedasticity in regression refers to a situation where the variability of the residuals (the differences between the observed values and the predicted values) is not constant across different levels of the independent variables
Heteroscedasticity can affect the model and its results in several ways:

Biased coefficient estimates: When heteroscedasticity is present, the ordinary least squares (OLS) regression assumes that the residuals have constant variance (homoscedasticity). However, heteroscedasticity violates this assumption. As a result, the OLS estimates can be biased, leading to inaccurate coefficient estimates.

Inefficient standard errors: Heteroscedasticity violates the assumption of homoscedasticity, which assumes that the error terms have constant variance. Consequently, the standard errors of the coefficient estimates can be inefficient and incorrect. This affects the calculation of t-statistics and p-values, leading to unreliable inference about the significance of the coefficients.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 19

1. Variable Selection: Remove one or more correlated variables from the regression model to eliminate multicollinearity. Prioritize variables that are theoretically more relevant or have stronger relationships with the dependent variable.

2. Data Collection: Collect additional data to reduce the correlation between variables. Increasing sample size can help alleviate multicollinearity by providing a more diverse range of observations.

3. Ridge Regression: Use regularization techniques like ridge regression to mitigate multicollinearity. Ridge regression introduces a penalty term that shrinks the coefficient estimates, reducing their sensitivity to multicollinearity.

4. Principal Component Analysis (PCA): Transform the correlated variables into a set of uncorrelated principal components through techniques like PCA. The principal components can then be used as independent variables in the regression model.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 20

Polynomial regression is a form of regression analysis that models the relationship between the dependent variable and independent variable(s) as an nth-degree polynomial function. Unlike linear regression, which assumes a linear relationship between the variables, polynomial regression can capture nonlinear relationships by including higher-order terms of the independent variable(s) in the model. The resulting regression equation becomes a polynomial equation.

It can be used when:
U-shaped or inverted U-shaped relationships: Polynomial regression can capture U-shaped or inverted U-shaped relationships between variables. For example, in economics, it can be used to model the relationship between the level of investment and economic growth, where there might be diminishing or increasing returns.

Nonlinear growth or decay patterns: Polynomial regression can handle situations where the relationship exhibits nonlinear growth or decay. It can be used to model growth rates, saturation levels, or decay rates in various fields such as biology, physics, or environmental sciences.

Higher-order interactions: Polynomial regression can accommodate higher-order interactions between variables. For example, if there are quadratic or cubic interactions between variables, polynomial regression can capture those complex relationships.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 21

A loss function, also known as a cost function or objective function, is a measure used to quantify the discrepancy or error between the predicted values and the true values in a machine learning or optimization problem. The choice of a suitable loss function depends on the specific task and the nature of the problem.
The purpose of a loss function in machine learning algorithms is to quantify the discrepancy or error between the predicted outputs and the true values in order to guide the learning process. Loss functions play a crucial role in training models by providing a measure of how well the model is performing and allowing optimization algorithms to adjust the model's parameters to minimize the error. Here are a few key purposes of loss functions in machine learning algorithms, along with examples:

1. Model Optimization:
Loss functions are used to optimize the parameters of a model during the training process. By minimizing the loss function, the model is adjusted to improve its predictive accuracy and capture meaningful patterns in the data.
2. Gradient Calculation:
Loss functions enable the calculation of gradients, which indicate the direction and magnitude of the steepest descent for optimization algorithms. Gradients provide information on how to update the model's parameters to minimize the loss.
3. Model Selection:
Loss functions aid in model selection and comparison. They provide a quantitative measure to evaluate and compare the performance of different models, allowing the selection of the most appropriate model for a given task.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 22

Convex loss function:

Shape: A convex loss function has a bowl-like shape, with a single global minimum. The curve of the loss function is such that any two points on the curve lie above the straight line segment connecting them.
Optimization: Convex loss functions are desirable for optimization because they guarantee the existence of a global minimum. This means that any local minimum of a convex loss function is also the global minimum, making optimization more reliable and efficient. 

Non-convex loss function:

Shape: A non-convex loss function does not have a single global minimum but can have multiple local minima, saddle points, or flat regions. 
Optimization: Non-convex loss functions present challenges for optimization because finding the global minimum is not guaranteed. Optimization algorithms may get stuck in local minima 
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 23

Squared loss, also known as Mean Squared Error (MSE), calculates the average of the squared differences between the predicted and true values. It penalizes larger errors more severely due to the squaring operation. The squared loss function is differentiable and continuous, which makes it well-suited for optimization algorithms that rely on gradient-based techniques.

It can be calculated as 
J(theta)=(y_true-y_pred)**2 * (1/n)
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 24
Absolute loss, also known as Mean Absolute Error (MAE), measures the average of the absolute differences between the predicted and true values. It treats all errors equally, regardless of their magnitude, making it less sensitive to outliers compared to squared loss. Absolute loss is less influenced by extreme values and is more robust in the presence of outliers.
It can be calculated as 
J(theta)=|(y_true-y_pred)| * (1/n)
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 25
Log loss, also known as cross-entropy loss, is a loss function commonly used in classification problems, particularly in binary or multi-class classification tasks. It measures the discrepancy between the predicted probabilities and the true class labels. Log loss quantifies the accuracy of the predicted probabilities by penalizing incorrect predictions and rewarding confident and accurate predictions. Lower log loss values indicate better model performance.
It can be calculated as 
Log Loss = -1/N *  [y * log(p) + (1-y) * log(1-p)]
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 26
Choosing an appropriate loss function for a given problem involves considering the nature of the problem, the type of learning task (regression, classification, etc.), and the specific goals or requirements of the problem.

1. Regression Problems:
For regression problems, where the goal is to predict continuous numerical values, common loss functions include:

- Mean Squared Error (MSE): This loss function calculates the average squared difference between the predicted and true values. It penalizes larger errors more severely.
 Mean Absolute Error (MAE): This loss function calculates the average absolute difference between the predicted and true values. It treats all errors equally and is less sensitive to outliers.

2. Classification Problems:
For classification problems, where the task is to assign instances into specific classes, common loss functions include:

- Binary Cross-Entropy (Log Loss): This loss function is used for binary classification problems, where the goal is to estimate the probability of an instance belonging to a particular class
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 27

Regularization, in the context of loss functions, refers to the technique of adding a penalty term to the loss function in order to prevent overfitting and improve the generalization performance of a machine learning model. Regularization helps control the complexity of the model and avoids excessive reliance on the training data, reducing the risk of overfitting.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 28

Huber loss, also known as the Huber-White loss or Huber penalty, is a loss function used in regression analysis that combines the advantages of both the mean absolute error (MAE) and the mean squared error (MSE) loss functions. It is particularly effective in handling outliers in the data.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 29

Quantile loss, also known as pinball loss or quantile regression loss, is a loss function used in quantile regression to estimate the conditional quantiles of a target variable. It measures the discrepancy between the predicted quantiles and the corresponding actual quantiles of the target variable. Quantile loss is particularly useful when the focus is on capturing the entire distribution of the target variable rather than just estimating the mean.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 30

Difference :
- Sensitivity to Errors: Squared loss penalizes larger errors more severely due to the squaring operation, while absolute loss treats all errors equally, regardless of their magnitude.
- Sensitivity to Outliers: Squared loss is more sensitive to outliers because the squared differences amplify the impact of extreme values. Absolute loss is less sensitive to outliers as it only considers the absolute differences.
- Differentiability: Squared loss is differentiable, making it suitable for gradient-based optimization algorithms. Absolute loss is not differentiable at zero, which may require specialized optimization techniques.
- Robustness: Absolute loss is more robust to outliers and can provide more robust estimates in the presence of extreme values compared to squared loss.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 31

In machine learning, an optimizer is an algorithm or method used to adjust the parameters of a model in order to minimize the loss function or maximize the objective function. Optimizers play a crucial role in training machine learning models by iteratively updating the model's parameters to improve its performance. They determine the direction and magnitude of the parameter updates based on the gradients of the loss or objective function.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 32

Gradient Descent is a popular optimization algorithm used in various machine learning models. It iteratively adjusts the model's parameters in the direction opposite to the gradient of the loss function. It continuously takes small steps towards the minimum of the loss function until convergence is achieved.

Working:
1. Initialization: Initialize the slope and intercept with random values or some predefined values.

2. Forward Pass: Compute the predicted values (ŷ) using the current slope and intercept.

3. Gradient Calculation: Calculate the gradients of the MSE loss function with respect to the slope and intercept.

4. Parameter Update: Update the slope and intercept using the gradients and the learning rate. Repeat this step until convergence.

5. Iteration: Repeat steps 2 to 4 for a fixed number of iterations or until the convergence criterion is met.

6. Convergence: Stop the algorithm when the loss function converges or when the desired level of accuracy is achieved. The final values of the slope and intercept represent the best-fit line that minimizes the loss function.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 33

There are different variants of gradient descent :

- Stochastic Gradient Descent (SGD): This variant randomly samples a subset of the training data (a batch) in each iteration, making the updates more frequent but with higher variance.

- Mini-Batch Gradient Descent: This variant combines the benefits of SGD and batch gradient descent by using a mini-batch of data for each parameter update.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 34

Gradient Descent determines the step size for parameter updates.A learning rate that is too small may result in slow convergence, while a learning rate that is too large can lead to overshooting or instability.

How to choose the right value:

1. Hyperparameter tuning with cross validation:
We can use gridsearch cv in order to find the correct value of learning rate.

2. Learning Rate Schedules:
Instead of using a fixed learning rate throughout the training process, you can employ learning rate schedules that dynamically adjust the learning rate over time. Some commonly used learning rate schedules include:

- Step Decay: The learning rate is reduced by a factor (e.g., 0.1) at predefined epochs or after a fixed number of iterations.

- Exponential Decay: The learning rate decreases exponentially over time.

- Adaptive Learning Rates: Techniques like AdaGrad, RMSprop, and Adam automatically adapt the learning rate based on the gradients, adjusting it differently for each parameter.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 35

GD can handle local optima in a few ways:

a) Random Initialization: GD can be initialized with different starting points to increase the chances of finding a better solution. By starting from different initial positions, GD can explore different regions of the search space and potentially escape local optima.

b) Learning Rate: Adjusting the learning rate can help GD overcome local optima. A smaller learning rate allows GD to take smaller steps, which increases the chances of finding a better solution by navigating around local optima. On the other hand, a larger learning rate enables GD to jump out of shallow local optima, but it may overshoot the global optimum.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 37

In Gradient Descent (GD), the batch size refers to the number of training examples used in each iteration to compute the gradient and update the model parameters. The choice of batch size has an impact on both the training process and the computational efficiency.
The impact of batch size on training includes the following:

Computational Efficiency: Larger batch sizes utilize parallelism more efficiently, as modern hardware accelerators like GPUs can process larger batches in parallel. Smaller batch sizes may lead to underutilization of hardware resources.

Generalization: Smaller batch sizes introduce more noise into the gradient estimate due to the limited number of samples. This noise can have a regularizing effect, preventing overfitting and potentially improving generalization performance. However, too small a batch size can make the optimization process unstable or slow.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 38

Momentum in optimization algorithms accumulates information from previous gradients to determine the direction and speed of the parameter updates.
Roles:
a) Faster Convergence: Momentum accelerates convergence by allowing the optimization algorithm to maintain a consistent direction when the gradients change rapidly. It helps overcome oscillations and shallow local optima by carrying the accumulated momentum in the parameter updates.

b) Smoother Optimization Trajectory: By reducing the oscillations and erratic behavior during optimization, momentum can result in a smoother trajectory towards the minimum. This can lead to faster convergence and potentially help the algorithm find better solutions.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 39
The main difference between Batch Gradient Descent (Batch GD), Mini-batch Gradient Descent (Mini-batch GD), and Stochastic Gradient Descent (SGD) lies in the size of the data used to compute the gradient and update the model parameters in each iteration.

Batch GD: Batch GD computes the gradient using the entire training dataset
Mini-batch GD: Mini-batch GD randomly samples a smaller subset (mini-batch) of the training data to compute the gradient and update the parameters. 

-----------------------------------------------------------------------------------------------------------------------------------------

Ans 40
Impacts:

Convergence Speed: The learning rate determines the size of the steps taken during each parameter update. A larger learning rate allows GD to take larger steps, which can result in faster convergence. However, if the learning rate is too large, it may cause overshooting, leading to instability or divergence of the optimization process. On the other hand, a smaller learning rate takes smaller steps, leading to slower convergence but with more stable updates.

Convergence Stability: Choosing an appropriate learning rate is essential to ensure convergence stability. If the learning rate is too high, the optimization process may oscillate around the minimum or fail to converge at all. On the contrary, if the learning rate is too small, GD may converge very slowly, taking a significant number of iterations to reach the desired solution. It is important to find a balance that allows for stable convergence without sacrificing convergence speed.



-----------------------------------------------------------------------------------------------------------------------------------------

Ans 41
Regularization is a technique used in machine learning to prevent overfitting and improve the generalization ability of a model. It introduces additional constraints or penalties to the loss function, encouraging the model to learn simpler patterns and avoid overly complex or noisy representations. Regularization helps strike a balance between fitting the training data well and avoiding overfitting, thereby improving the model's performance on unseen data.


-----------------------------------------------------------------------------------------------------------------------------------------

Ans 42

Differenc btw L1 and L2
L1 is also known as Lasso while L2 is also known as Ridge

1. Penalty Term:
L1 Regularization (Lasso Regularization):
L1 regularization adds a penalty term to the loss function that is proportional to the sum of the absolute values of the model's coefficients. The penalty term encourages sparsity, meaning it tends to set some coefficients exactly to zero.

L2 Regularization (Ridge Regularization):
L2 regularization adds a penalty term to the loss function that is proportional to the sum of the squared values of the model's coefficients. The penalty term encourages smaller magnitudes of all coefficients without forcing them to zero.

2. Effects on Coefficients:
L1 Regularization:
L1 regularization encourages sparsity by setting some coefficients to exactly zero. It performs automatic feature selection, effectively excluding less relevant features from the model. This makes L1 regularization useful when dealing with high-dimensional feature spaces or when there is prior knowledge that only a subset of features is important.

L2 Regularization:
L2 regularization encourages smaller magnitudes for all coefficients without enforcing sparsity. It reduces the impact of less important features but rarely sets coefficients exactly to zero. L2 regularization helps prevent overfitting by reducing the sensitivity of the model to noise or irrelevant features. It promotes a more balanced influence of features in the model.
----------------------------------------------------------------------------------------------------------------------------------------

Ans 43
L2 regularization, also known as Ridge regularization, adds a penalty term to the loss function that is proportional to the sum of the squared values of the model's coefficients. It encourages smaller magnitudes of all coefficients without forcing them to zero
Equation= Loss function + lambda * ||coefficients|| **2
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 44
Elastic net regularization is a regularization technique that is a mixture of both L1 (Lasso) and L2 (Ridge) regularlization in order to address the limitations of each individually. It is particularly useful when dealing with high-dimensional datasets with correlated features, where Lasso alone may select only one feature among a group of highly correlated features.

Equation= Loss function + lambda * ||coefficients|| **2 + lamda * |coefficient|
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 45
Regularization helps prevent overfitting in machine learning models by introducing a penalty term to the objective function during training. Overfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data. 
Therefore it helps in preventing overfitting so that our model performs well on unseen data.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 46
Early stopping is a technique used in machine learning, particularly in the training of neural networks, to prevent overfitting and improve generalization. It is closely related to regularization as it provides a form of implicit regularization.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 47
Dropout regularization is a technique commonly used in neural networks to prevent overfitting. It involves randomly "dropping out" a proportion of units (neurons) in a neural network layer during each training iteration.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 48
In finding the best parameters we can perform hyperparameter tuning with cross validation.

1. Grid Search:
Grid search is a commonly used technique to select the regularization parameter. It involves specifying a range of potential values for λ and evaluating the model's performance using each value. The performance metric can be measured on a validation set or using cross-validation. The regularization parameter that yields the best performance (e.g., highest accuracy, lowest mean squared error) is then selected as the optimal value.

2. Cross-Validation:
Cross-validation is a robust technique for model evaluation and parameter selection. It involves splitting the dataset into multiple subsets or folds, training the model on different combinations of the subsets, and evaluating the model's performance. The regularization parameter can be selected based on the average performance across the different folds.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 49
Feature selection focuses on selecting a subset of features to use in the model, while regularization focuses on constraining the model's parameters during training.

-----------------------------------------------------------------------------------------------------------------------------------------

Ans 50
Regularized models involve a trade-off between bias and variance. Bias refers to the error introduced by approximating a real-world problem with a simplified model, whereas variance refers to the model's sensitivity to fluctuations in the training data.

-----------------------------------------------------------------------------------------------------------------------------------------

Ans 51
Its aim is to find the best hyperpane which seprates the data.It can be used when the data is not linearly seprable.
Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression problems. It is particularly effective for solving binary classification problems but can be extended to handle multi-class classification as well. SVM aims to find an optimal hyperplane that maximally separates the classes or minimizes the regression error. 

Working:
1. Hyperplane:
In SVM, a hyperplane is a decision boundary that separates the data points belonging to different classes. In a binary classification scenario, the hyperplane is a line in a two-dimensional space, a plane in a three-dimensional space, and a hyperplane in higher-dimensional spaces. The goal is to find the hyperplane that best separates the classes.

2. Support Vectors:
Support vectors are the data points that are closest to the decision boundary or lie on the wrong side of the margin. These points play a crucial role in defining the hyperplane. SVM algorithm focuses only on these support vectors, making it memory efficient and computationally faster than other algorithms.

3. Margin:
The margin is the region between the support vectors of different classes and the decision boundary. SVM aims to find the hyperplane that maximizes the margin, as a larger margin generally leads to better generalization performance. SVM is known as a margin-based classifier.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 52
With the help of kernels we can transform lower dimensional not seprable data into higher dimensional.The kernel trick is a technique used in Support Vector Machines (SVM) to handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional space. It allows SVM to find a linear decision boundary in the transformed feature space without explicitly computing the coordinates of the transformed data points.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 53
Support vecors plays a vital role in finding the best hyperplane which divdes out dataset into two regions.Support vectors are the
closest datapoint from the hyperplane.These points play a crucial role in defining the decision boundary. The margin ensures that the decision boundary is determined by the support vectors, rather than being influenced by other data points. SVM focuses on optimizing the position of the decision boundary with respect to the support vectors, leading to a more effective classification.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 54
The margin in Support Vector Machines (SVM) is a critical concept that plays a crucial role in determining the optimal decision boundary between classes. The purpose of the margin is to maximize the separation between the support vectors of different classes and the decision boundary. 
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 55
Handling unbalanced datasets:
1. Class Weighting:
One common approach is to assign different weights to the classes during training. This adjusts the importance of each class in the optimization process and helps SVM give more attention to the minority class. The weights are typically inversely proportional to the class frequencies in the training set.
2. Oversampling:
Oversampling the minority class involves increasing its representation in the training set by duplicating or generating new samples. This helps to balance the class distribution and provide the classifier with more instances to learn from.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 56
Linear SVM: When the data is linearly seprable Linear SVM can be used.Linear SVM assumes that the data can be perfectly separated by a linear decision boundary. It aims to find the best hyperplane that maximizes the margin between the classes

Non-linear SVM: When the data is not linearly seprable Non Linear SVM can be used.Non-linear SVM can handle datasets that are not linearly separable by using a technique called the kernel trick. The kernel trick maps the input data into a higher-dimensional feature space where the classes may become separable by a hyperplane
eg of kernels: sigmoid kernel,RBF kernel
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 57
The C-parameter in SVM represents the regularization parameter or the penalty parameter for misclassifications. It controls the trade-off between achieving a larger margin and allowing misclassifications in the training da
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 58
The concept of slack variables is based on the idea that each data point can have a penalty associated with it if it lies on the wrong side of the margin or misclassified. The objective of SVM is to minimize the sum of slack variables while maximizing the margin. The slack variables allow for a trade-off between achieving a larger margin and allowing misclassifications, effectively providing a soft margin between the classes.

-----------------------------------------------------------------------------------------------------------------------------------------

Ans 59
Hard Margin SVM: Hard margin SVM assumes that the data is linearly separable without any misclassifications. It aims to find the optimal hyperplane that perfectly separates the classes while maximizing the margin between the classes.
 
Soft Margin SVM: Soft margin SVM allows for a certain degree of misclassification or violation of the margin constraints. It recognizes that real-world datasets may contain some overlap or noise, and a perfectly separable solution may not exist. 
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 60
Interpreting the coefficients in an SVM model can provide insights into which features are more influential in making predictions. Features with larger absolute coefficients have a stronger impact on the decision boundary, meaning they contribute more to the classification. 
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 61
Decision tree is just same as nested if else statement. A decision tree is a supervised machine learning algorithm that is used for both classification and regression tasks. It represents a flowchart-like structure where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a prediction.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 62
A decision tree makes splits or determines the branching points based on the attribute that best separates the data and maximizes the information gain or reduces the impurity.

1. Information Gain:
Information gain is a commonly used criterion for splitting in decision trees. It measures the reduction in uncertainty or entropy in the target variable achieved by splitting the data based on a particular attribute. The attribute that results in the highest information gain is selected as the splitting attribute.

2. Gini Impurity:
Another criterion is Gini impurity, which measures the probability of misclassifying a randomly selected element from the dataset if it were randomly labeled according to the class distribution. The attribute that minimizes the Gini impurity is chosen as the splitting attribute.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 63
1. Information Gain:
Information gain is a commonly used criterion for splitting in decision trees. It measures the reduction in uncertainty or entropy in the target variable achieved by splitting the data based on a particular attribute. The attribute that results in the highest information gain is selected as the splitting attribute.

2. Gini Impurity:
Another criterion is Gini impurity, which measures the probability of misclassifying a randomly selected element from the dataset if it were randomly labeled according to the class distribution. The attribute that minimizes the Gini impurity is chosen as the splitting attribute.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 64
Information gain is a commonly used criterion for splitting in decision trees. It measures the reduction in uncertainty or entropy in the target variable achieved by splitting the data based on a particular attribute. The attribute that results in the highest information gain is selected as the splitting attribute.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 65
Imputation:The missing values can be replaced by mean,median or mode of the variable.Imputation replaces missing values with a substituted value based on statistical techniques or domain knowledge. Common imputation methods include mean imputation, median imputation, mode imputation, or regression imputation.

Ignore Missing Values:Another option is to ignore the missing values and treat them as a separate category or class. This approach can be suitable when missing values have a unique meaning or when the missingness itself is informative. The decision tree algorithm can create a separate branch for missing values during the splitting process.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 66 Pruning basically mean cutting our tree before it gets to its max depth. Pruning is important bcs if we dont perform pruning that
our model will perform very well on training data but when new data will come then its performance will degrade.It is a technique used in decision trees to reduce overfitting and improve the model's generalization performance. It involves the removal or simplification of specific branches or nodes in the tree that may be overly complex or not contributing significantly to the overall predictive power. Pruning helps prevent the decision tree from becoming too specific to the training data, allowing it to better generalize to unseen data.

Types of pruning
Pre Pruning: This can be done with the help of hyperparemeter tuning.It involves stopping the growth of the decision tree before it reaches its maximum potential.
Post Pruning: Post pruning involves building the decision tree to its maximum potential and then selectively removing or collapsing certain branches or nodes.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 67
A classification tree is used for classification tasks, assigning data points to classes or categories, while a regression tree is used for regression tasks, predicting continuous numerical values.

classification tree : It recursively splits the data based on feature values to create decision rules that assign data points to different classes or categories.

regression tree : It also recursively splits the data based on feature values, but instead of classifying data into categories, it predicts continuous numerical values
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 68
The interpretation of decision boundaries in a decision tree depends on the feature and threshold used at each split. At each node, the decision tree evaluates a specific feature and compares its value to a threshold. Based on this comparison, the data is directed to either the left or right child node, representing different branches or decisions.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 69
Feature importance helps in feature selection and engineering in order to find the most suitable columns for our model. It can be determined using various metrics, such as Gini impurity, information gain, or mean decrease impurity. These metrics assess the quality of a split based on a particular feature and its ability to separate the data into distinct classes or reduce the overall impurity within the tree.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 70
In most of the ensemble techniques we primarily use Decision tree as base learners. Decision trees are often used as the building blocks or base models within ensemble techniques. This is because decision trees have desirable properties, such as flexibility, interpretability, and the ability to handle both categorical and numerical data. By combining multiple decision trees, ensemble techniques can overcome the limitations of individual decision trees and enhance their predictive power.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 71
Ensemble techniques are machine learning methods that combine multiple models to make predictions or decisions.Ensemble methods leverage the concept of "wisdom of the crowd," where the collective decision-making of multiple models can outperform any single model.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 72
Bagging (Bootstrap Aggregating) is an ensemble technique in machine learning that involves training multiple instances of the same base model on different subsets of the training data. These models are then combined through averaging or voting to make the final prediction. Bagging helps reduce overfitting and improves the stability and accuracy of the model.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 73
Bootstrapping involves creating multiple training datasets by randomly sampling the original training dataset with replacement.
It is important bcs we need to give the dataset to our models parallely.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 74
Boosting focuses on sequentially building an ensemble by training weak models that learn from the mistakes of previous models. Each subsequent model gives more weight to misclassified instances, leading to improved performance.

Boosting Process:

- Initial Model: The process starts with an initial base model (weak learner) trained on the entire training dataset.

- Weighted Instances: Each instance in the training dataset is assigned an initial weight, which is typically set uniformly across all instances.

- Iterative Learning: The subsequent models are trained iteratively, with each model learning from the mistakes of the previous models. In each iteration:

  a. Model Training: A weak learner is trained on the training dataset, where the weights of the instances are adjusted to give more emphasis to the misclassified instances from previous iterations.

  b. Instance Weight Update: After training the model, the weights of the misclassified instances are increased, while the weights of the correctly classified instances are decreased. This puts more focus on the difficult instances to improve their classification.

- Model Weighting: Each weak learner is assigned a weight based on its performance in classifying the instances. The better a model performs, the higher its weight.

- Final Prediction: The predictions of all the weak learners are combined, typically using a weighted voting scheme, to make the final prediction.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 75
AdaBoost: AdaBoost focuses on improving the model's performance by sequentially adjusting the weights of misclassified samples in each iteration. It trains weak learners in a stage-wise manner, where each subsequent weak learner focuses more on the samples that were misclassified by previous learners. The final prediction is obtained by combining the predictions of all weak learners, with each learner assigned a weight based on its performance.

Gradient Boosting: Gradient Boosting, on the other hand, builds the ensemble by sequentially training weak learners to correct the mistakes made by previous learners. 
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 76
The main purposes of using Random Forests are:

Prediction Accuracy: By combining the predictions of multiple decision trees, Random Forests aim to improve prediction accuracy. The ensemble of trees helps to reduce bias and variance, capture different aspects of the data, and make robust predictions.
Overfitting Reduction: Random Forests are effective in reducing overfitting because they utilize bootstrapping and feature subsampling. Each decision tree is trained on a subset of the data, and the combination of these trees helps to counterbalance the tendency of individual trees to overfit to specific patterns in the data.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 77
Feature importance in Random Forests can be obtained by measuring the mean decrease impurity (Gini importance) or the mean decrease in accuracy (permutation importance) caused by each feature. The higher the feature importance value, the more influential the feature is in making predictions.
-----------------------------------------------------------------------------------------------------------------------------------------

Ans 78
Stacking, also known as stacked generalization, is an ensemble learning technique that combines the predictions of multiple models (often different types of models) to make final predictions. It aims to improve prediction performance by learning a meta-model that combines the strengths of the individual base models.

-----------------------------------------------------------------------------------------------------------------------------------------

Ans 79
Advantages:

Improved Predictive Performance
Reduction of Overfitting
Robustness
Flexibility

Disadvantages:
Increased Complexity: 
Interpretability: 
Overfitting Risk: 


-----------------------------------------------------------------------------------------------------------------------------------------

Ans 80
Bias-Variance Trade-off: Increasing the number of models generally reduces bias but can increase variance. Initially, as the number of models increases, the ensemble's prediction performance improves due to increased diversity among the models.

Validation and Testing Performance: The optimal number of models can be determined by monitoring the performance on a validation dataset or through cross-validation.



















