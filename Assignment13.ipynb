{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2a598b-4987-4f7c-bb4a-932aaa589dee",
   "metadata": {},
   "source": [
    "## Ans1 a)\n",
    "\n",
    "Identify Data Sources: Determine the different sources from which you want to collect data. This can include databases, APIs, message queues, log files, or streaming platforms.\n",
    "\n",
    "Define Data Collection Strategy: Decide on the frequency and method of data collection for each source. This can involve scheduled batch processing, event-driven processing, or real-time streaming.\n",
    "\n",
    "Extract Data: Develop connectors or APIs to extract data from each source. Use appropriate libraries, APIs, or query languages specific to the data source, such as SQL for databases or REST APIs for web services.\n",
    "\n",
    "Transform and Normalize Data: Perform any necessary data transformation and normalization to ensure consistency and compatibility across different sources. This may involve data type conversions, merging or splitting data, or handling missing values.\n",
    "\n",
    "Apply Data Validation and Cleansing: Implement validation and cleansing steps to ensure the integrity and quality of the collected data. This can include data validation rules, data type validation, duplicate removal, or outlier detection.\n",
    "\n",
    "Store Data: Determine the storage mechanism based on your requirements. This can involve relational or NoSQL databases, data lakes, or cloud storage services like Amazon S3 or Google Cloud Storage.\n",
    "\n",
    "Data Partitioning and Indexing: Implement strategies to partition and index the data based on your retrieval and analysis needs. Partitioning can be based on time, location, or any other relevant criteria, while indexing improves query performance.\n",
    "\n",
    "Error Handling and Retry Mechanism: Implement error handling mechanisms to capture and handle any exceptions that occur during data ingestion. Set up a retry mechanism to handle temporary failures or connectivity issues.\n",
    "\n",
    "Monitoring and Alerting: Implement monitoring tools and metrics to track the health and performance of the data ingestion pipeline. Set up alerts to notify administrators of any issues or anomalies, such as data source unavailability or data ingestion failures.\n",
    "---------------------------------------------------------------------------------------------------------------------------------------\n",
    "## b)\n",
    "\n",
    "IoT Device Integration: Establish a connection with the IoT devices to collect sensor data. This may involve using protocols such as MQTT or CoAP and setting up appropriate device registries or management platforms.\n",
    "\n",
    "Data Streaming: Set up a real-time data streaming framework to handle the continuous flow of sensor data. Popular streaming platforms like Apache Kafka or AWS Kinesis can be used for this purpose.\n",
    "\n",
    "Data Serialization and Encoding: Choose an appropriate data serialization format such as Avro, Protocol Buffers, or JSON to encode the sensor data for streaming.\n",
    "\n",
    "Real-time Processing: Implement real-time processing components to analyze and transform the incoming sensor data. This can involve performing aggregations, filtering, feature extraction, or applying machine learning models in real-time.\n",
    "\n",
    "Data Storage and Persistence: Store the processed sensor data in a suitable storage system. Depending on the use case, this can be a database, data warehouse, or data lake.\n",
    "\n",
    "Data Visualization and Monitoring: Develop visualizations or dashboards to provide real-time insights and monitoring of the sensor data. This allows stakeholders to track the system's performance and make informed decisions based on the collected data.\n",
    "----------------------------------------------------------------------------------------------------------------------------------------\n",
    "## c)\n",
    "\n",
    "File Format Detection: Implement a mechanism to detect the file format of incoming data files. This can be done based on file extensions or by inspecting the file content.\n",
    "\n",
    "Data Parsing: Develop parsers or libraries to parse and extract data from different file formats. Use appropriate libraries or APIs for each format, such as CSV parsers or JSON deserializers.\n",
    "\n",
    "Data Validation: Apply data validation rules to ensure the integrity and quality of the ingested data. This can include checking for data type conformity, enforcing data constraints, or performing format-specific validation.\n",
    "\n",
    "Data Cleansing and Transformation: Implement data cleansing and transformation steps to handle inconsistencies or errors in the ingested data. This can involve data cleaning techniques like removing duplicates, handling missing values, or standardizing data formats.\n",
    "\n",
    "Schema Mapping: Define a schema or mapping mechanism to map the incoming data to a standardized format or schema. This helps ensure consistency across different data sources and simplifies downstream processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75110271-d6af-4ffa-a36e-b5db4cf84a93",
   "metadata": {},
   "source": [
    "## Ans 3 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66efe3d-63c4-47f5-b31c-68d7fe3715f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bd1e87-3837-42a1-b71a-95451be973a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset=fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb59f8d-5642-4c6c-b1a3-4f367463af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dataset.data,columns=dataset.feature_names)\n",
    "df['Price']=dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12be5667-52f4-43a1-9ed1-fd7ca0bb0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,0:8]\n",
    "y=df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9732e0-79ce-4178-9832-593e0542444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000490693530499\n"
     ]
    }
   ],
   "source": [
    "## Cross Validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=67)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor=LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score=cross_val_score(LinearRegression(),X=X_train,y=y_train,cv=5)\n",
    "print(np.mean(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0a595-5e78-4987-ae67-331699cfcfe1",
   "metadata": {},
   "source": [
    "## Ans 3 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "445b7231-01d1-4b2f-81f6-3a098f778bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "independent_features,target=make_classification(n_samples=500,n_features=3,n_informative=1,n_classes=2,n_clusters_per_class=1,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e57e4dd-136a-4389-8896-6ad73e5bbc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df=pd.DataFrame(independent_features,columns=['f1','f2','f3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f70b0a6-819e-4d93-b216-1c43d7d643ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df['target']=target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7826e6a4-094e-4fd4-99d9-0e99cc3ebf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=clf_df.drop('target',axis=1)\n",
    "y=clf_df.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a0350fc-b933-4ad4-8f3a-0f0fca690eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0\n",
      "f1score 1.0\n",
      "precision 1.0\n",
      "recall 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "clf=KNeighborsClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "f1score=f1_score(y_test,y_pred)\n",
    "precision=precision_score(y_test,y_pred)\n",
    "recall=recall_score(y_test,y_pred)\n",
    "print('accuracy',accuracy)\n",
    "print('f1score',f1score)\n",
    "print('precision',precision)\n",
    "print('recall',recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f2239-1d1d-46a7-ba16-cf694b4621f6",
   "metadata": {},
   "source": [
    "## Ans3 c)\n",
    "Dataset Analysis: Analyze the class distribution of the imbalanced dataset to understand the extent of the imbalance. Identify the minority class (positive class) and majority class (negative class).\n",
    "\n",
    "Stratified Sampling Approach: Implement stratified sampling to create training and validation sets. This approach ensures that the class distribution is maintained in both sets, providing a balanced representation of the classes.\n",
    "\n",
    "Determine Sampling Ratio: Decide on the appropriate ratio for stratified sampling. The ratio can be set based on the severity of the class imbalance, ensuring that both classes have sufficient representation in the training and validation sets.\n",
    "\n",
    "Random Sampling: Randomly sample instances from each class while maintaining the predetermined ratio. This ensures that the selected samples are representative of the original class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54730-e185-49cd-8cd8-b2e083f53914",
   "metadata": {},
   "source": [
    "## Ans4 a)\n",
    "Infrastructure Selection: Choose a suitable infrastructure for deploying the model. This can include cloud platforms, on-premises servers, or serverless architectures, depending on your requirements and scalability needs.\n",
    "\n",
    "Real-time Data Ingestion: Set up a mechanism to capture and ingest user interactions or events in real-time. This can involve integrating with APIs, message queues, or streaming platforms to receive user data as it happens.\n",
    "\n",
    "Model Deployment: Deploy the trained machine learning model to the chosen infrastructure. This can be done using frameworks like Flask, Django, or serverless computing platforms such as AWS Lambda or Azure Functions.\n",
    "----------------------------------------------------------------------------------------------------------------------------------------\n",
    "## b)\n",
    "Version Control: Use a version control system like Git to manage the machine learning model's code and associated configuration files. Maintain separate branches for development, testing, and production environments.\n",
    "\n",
    "Continuous Integration and Deployment (CI/CD): Set up a CI/CD pipeline to automate the deployment process. Configure triggers that initiate the pipeline whenever new code changes are pushed to the repository or based on predefined schedules.\n",
    "\n",
    "Build and Packaging: Create a build script that packages the model code, dependencies, and any necessary configurations into deployable artifacts. Use tools like Docker or serverless frameworks (e.g., AWS SAM or Azure Functions) to encapsulate the model into containerized or serverless deployments.\n",
    "----------------------------------------------------------------------------------------------------------------------------------------\n",
    "## c)\n",
    "Performance Monitoring: Set up monitoring systems to track the deployed model's performance metrics, including response times, resource utilization, throughput, and error rates. Use tools like Prometheus, Grafana, or cloud provider monitoring services to collect and visualize these metrics.\n",
    "\n",
    "Log Collection and Analysis: Collect logs from the deployed model and associated infrastructure components. Use log aggregation tools like ELK Stack (Elasticsearch, Logstash, Kibana) or cloud-native logging services to centralize and analyze logs for troubleshooting and performance optimization.\n",
    "\n",
    "Anomaly Detection: Apply anomaly detection techniques to identify unusual patterns or behaviors in the model's performance. Use statistical methods, machine learning algorithms, or anomaly detection platforms to automatically detect and alert on abnormal system behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b72353-1dea-444e-af4d-0f017fae0729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
